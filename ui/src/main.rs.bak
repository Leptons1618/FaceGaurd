use dioxus::prelude::*;
use faceguard_core::{camera, detection, events, recognition, tracking};
use gloo_storage::{LocalStorage, Storage};
use gloo_timers::callback::Interval;
use js_sys::Array;
use serde_json;
use wasm_bindgen::JsCast;
use wasm_bindgen::JsValue;
use wasm_bindgen_futures::{spawn_local, JsFuture};
use web_sys::{
    Blob, CanvasRenderingContext2d, HtmlCanvasElement, HtmlElement, HtmlVideoElement, ImageData,
    MediaStream, MediaStreamConstraints, Url,
};

// Global state keys for persistence
const IDENTITY_DB_KEY: &str = "faceguard_identities";
const EVENT_LOG_KEY: &str = "faceguard_events";
const SETTINGS_KEY: &str = "faceguard_settings";

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
struct AppSettings {
    detection_threshold: f64,
    recognition_threshold: f64,
    enable_tracking: bool,
    nms_threshold: f32,
    max_track_age_ms: u64,
}

impl Default for AppSettings {
    fn default() -> Self {
        Self {
            detection_threshold: 0.7,
            recognition_threshold: 0.85,
            enable_tracking: true,
            nms_threshold: 0.4,
            max_track_age_ms: 3000,
        }
    }
}

const GLOBAL_STYLE: &str = r#"
    :root {
        color: #e7ecf3;
        background: radial-gradient(circle at 20% 20%, #1f2a3a 0, #0d1320 35%),
                     radial-gradient(circle at 80% 0%, #143149 0, #0d1320 30%),
                     #0d1320;
        font-family: "Space Grotesk", "Manrope", "Inter", system-ui, -apple-system, sans-serif;
    }
    * { box-sizing: border-box; }
    body { margin: 0; background: transparent; }
    .page { max-width: 1100px; margin: 0 auto; padding: 32px 20px 48px; }
    .hero { display: flex; align-items: center; justify-content: space-between; gap: 12px; margin-bottom: 24px; }
    .hero h1 { margin: 0; font-size: 28px; letter-spacing: -0.5px; }
    .hero p { margin: 4px 0 0; color: #9fb3c8; }
    .card { background: rgba(255,255,255,0.04); border: 1px solid rgba(255,255,255,0.08); border-radius: 16px; padding: 18px 18px 16px; backdrop-filter: blur(8px); box-shadow: 0 20px 60px rgba(0,0,0,0.35); }
    .grid { display: grid; grid-template-columns: 1.15fr 0.85fr; gap: 18px; }
    .video-card video { width: 100%; border-radius: 12px; border: 1px solid rgba(255,255,255,0.1); background: #05070d; object-fit: cover; min-height: 320px; }
    .section-title { display: flex; align-items: center; justify-content: space-between; margin: 0 0 10px; }
    .pill { display: inline-flex; align-items: center; gap: 8px; padding: 6px 10px; border-radius: 999px; background: rgba(255,255,255,0.08); color: #b9c8d8; font-size: 13px; }
    .list { list-style: none; padding: 0; margin: 0; display: grid; gap: 8px; }
    .list li { padding: 10px 12px; border: 1px solid rgba(255,255,255,0.08); border-radius: 12px; background: rgba(255,255,255,0.02); color: #dbe5f2; }
    .muted { color: #94a9c3; }
    .events { margin-top: 22px; }
    .controls { display: flex; flex-wrap: wrap; gap: 10px; align-items: center; margin-bottom: 12px; }
    select { background: rgba(255,255,255,0.08); color: #e7ecf3; border: 1px solid rgba(255,255,255,0.14); border-radius: 10px; padding: 8px 12px; }
    button { background: linear-gradient(135deg, #3fb5ff, #7b74ff); border: none; color: white; padding: 10px 14px; border-radius: 12px; cursor: pointer; font-weight: 600; letter-spacing: 0.1px; }
    button.secondary { background: rgba(255,255,255,0.08); border: 1px solid rgba(255,255,255,0.14); }
    .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(140px,1fr)); gap: 10px; margin-top: 6px; }
    .stat { padding: 12px; border-radius: 12px; border: 1px solid rgba(255,255,255,0.08); background: rgba(255,255,255,0.03); }
    .stat span { display: block; color: #9fb3c8; font-size: 13px; }
    .stat strong { font-size: 22px; }
    @media (max-width: 900px) { .grid { grid-template-columns: 1fr; } }
    nav { display: flex; gap: 4px; margin-bottom: 20px; border-bottom: 1px solid rgba(255,255,255,0.1); padding-bottom: 12px; }
    nav a { padding: 10px 18px; border-radius: 10px; text-decoration: none; color: #9fb3c8; font-weight: 500; transition: all 0.2s; }
    nav a:hover { background: rgba(255,255,255,0.08); color: #e7ecf3; }
    nav a.active { background: rgba(63,181,255,0.15); color: #3fb5ff; border: 1px solid rgba(63,181,255,0.3); }
    .video-container { position: relative; }
    .video-overlay { position: absolute; top: 0; left: 0; pointer-events: none; }
"#;

#[derive(Debug, Clone, Copy, PartialEq)]
enum Page {
    Dashboard,
    LiveFeed,
    Events,
    Settings,
}

fn main() {
    launch(app);
}

fn app() -> Element {
    let mut current_page = use_signal(|| Page::Dashboard);

    rsx! {
        style { "{GLOBAL_STYLE}" }
        div { class: "page",
            nav {
                a {
                    class: if current_page() == Page::Dashboard { "active" } else { "" },
                    onclick: move |_| current_page.set(Page::Dashboard),
                    "Dashboard"
                }
                a {
                    class: if current_page() == Page::LiveFeed { "active" } else { "" },
                    onclick: move |_| current_page.set(Page::LiveFeed),
                    "Live Feed"
                }
                a {
                    class: if current_page() == Page::Events { "active" } else { "" },
                    onclick: move |_| current_page.set(Page::Events),
                    "Events"
                }
                a {
                    class: if current_page() == Page::Settings { "active" } else { "" },
                    onclick: move |_| current_page.set(Page::Settings),
                    "Settings"
                }
            }
            
            match current_page() {
                Page::Dashboard => rsx! { Dashboard {} },
                Page::LiveFeed => rsx! { LiveFeed {} },
                Page::Events => rsx! { EventsPage {} },
                Page::Settings => rsx! { Settings {} },
            }
        }
    }
}

#[component]
fn Dashboard() -> Element {
    let identity_db = use_signal(|| load_identity_db());
    let event_log = use_signal(|| load_event_log());
    let identities = identity_db().get_all();
    let events_data = event_log().get_recent(50);

    rsx! {
        header { class: "hero",
            div {
                h1 { "FaceGuard — Dashboard" }
                p { "System overview and quick stats" }
            }
            span { class: "pill", "Live" }
        }

        div { class: "stats",
            div { class: "stat", span { "Identities" } strong { "{identities.len()}" } }
            div { class: "stat", span { "Total Events" } strong { "{event_log().get_all().len()}" } }
            div { class: "stat", span { "Recent Events" } strong { "{events_data.len()}" } }
            div { class: "stat", span { "System" } strong { "Active" } }
        }

        section { class: "card", style: "margin-top: 20px;",
            h3 { "Recent Identities" }
            ul { class: "list",
                for ident in identities.iter() {
                    li { "{ident.name} · ID {ident.id} · conf {ident.confidence:.2}" }
                }
                if identities.is_empty() {
                    li { class: "muted", "No identities detected yet" }
                }
            }
        }
    }
}

#[component]
fn LiveFeed() -> Element {
    let mut detections = use_signal(|| vec![]);
    let mut tracks = use_signal(|| vec![]);
    let mut frame_count = use_signal(|| 0);
    let mut fps = use_signal(|| 0.0);
    let mut tracker = use_signal(|| tracking::Tracker::new(0.4, 3000));
    let _identity_db = use_signal(|| load_identity_db());
    let mut event_log = use_signal(|| load_event_log());

    use_effect(move || {
        spawn_local(async move {
            if let Err(err) = start_camera("camera-feed").await {
                web_sys::console::error_1(&err);
            }
        });

        let mut last_time = js_sys::Date::now();
        let _interval = Interval::new(100, move || {
            frame_count.set(frame_count() + 1);
            
            // Calculate FPS
            let now = js_sys::Date::now();
            let delta = (now - last_time) / 1000.0;
            if delta > 0.0 {
                fps.set(1.0 / delta);
            }
            last_time = now;
            
            // Process frame and detect faces
            if let Some(dets) = detect_faces_from_video("camera-feed", "temp-canvas") {
                // Apply NMS to remove overlapping detections
                let filtered_dets = detection::apply_nms(dets, 0.4);
                
                // Update tracker
                let timestamp = now as u64;
                let mut tracker_mut = tracker.write();
                let active_tracks = tracker_mut.update(filtered_dets.clone(), timestamp);
                
                // Store results
                detections.set(filtered_dets);
                tracks.set(active_tracks.clone());
                
                // Generate events for new tracks
                let mut event_log_mut = event_log.write();
                for track in &active_tracks {
                    if track.frames_tracked == 1 {
                        // New face detected
                        event_log_mut.add_event(
                            events::EventType::FaceDetected,
                            format!("Track #{}", track.track_id),
                            track.detection.confidence,
                            Some(track.track_id),
                        );
                    }
                }
                
                // Save event log
                save_event_log(&event_log());
            }
            
            // Draw overlays
            draw_detections_and_tracks("camera-feed", "overlay-canvas", &detections(), &tracks());
        });
    });

    camera::ingest();

    rsx! {
        header { class: "hero",
            div {
                h1 { "Live Feed" }
                p { "Real-time detection, tracking, and recognition · {fps():.1} FPS" }
            }
            span { class: "pill", "Frame: {frame_count} · {tracks().len()} tracks" }
        }

        section { class: "card video-card",
            div { class: "section-title",
                h2 { "Camera Feed" }
                span { class: "pill", "{detections().len()} faces · {tracks().len()} tracked" }
            }
            div { class: "video-container",
                video {
                    id: "camera-feed",
                    autoplay: true,
                    playsinline: true,
                    muted: true,
                    controls: false,
                    width: "100%",
                }
                canvas {
                    id: "overlay-canvas",
                    class: "video-overlay",
                }
                canvas {
                    id: "temp-canvas",
                    style: "display: none;",
                }
            }
        }

        section { class: "card", style: "margin-top: 20px;",
            h3 { "Active Tracks" }
            ul { class: "list",
                for track in tracks().iter() {
                    li {
                        "Track #{track.track_id} · {track.frames_tracked} frames · conf {track.detection.confidence:.2}"
                        if let Some(identity_id) = track.identity_id {
                            " · Identity #{identity_id}"
                        }
                    }
                }
                if tracks().is_empty() {
                    li { class: "muted", "No active tracks" }
                }
            }
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq)]
enum EventFilter {
    All,
    Alerts,
    Unknowns,
    Blacklisted,
}

#[component]
fn EventsPage() -> Element {
    let filter = use_signal(|| EventFilter::All);
    let event_log = use_signal(|| load_event_log());
    let events_data = event_log().get_all();

    let export_csv = {
        let filter = filter.clone();
        let events_data = events_data.clone();
        move |_| export_events_csv(filter(), &events_data)
    };

    let export_json = {
        let filter = filter.clone();
        let events_data = events_data.clone();
        move |_| export_events_json(filter(), &events_data)
    };

    rsx! {
        header { class: "hero",
            div {
                h1 { "Events" }
                p { "Filter, preview, and export security events" }
            }
        }

        section { class: "card",
            div { class: "controls",
                label { "Filter:" }
                select {
                    value: format!("{:?}", filter()),
                    onchange: move |e| {
                        let mut f = filter;
                        match e.value().as_str() {
                            "All" => f.set(EventFilter::All),
                            "Alerts" => f.set(EventFilter::Alerts),
                            "Unknowns" => f.set(EventFilter::Unknowns),
                            "Blacklisted" => f.set(EventFilter::Blacklisted),
                            _ => {}
                        }
                    },
                    option { value: "All", selected: filter() == EventFilter::All, "All" }
                    option { value: "Alerts", selected: filter() == EventFilter::Alerts, "Alerts" }
                    option { value: "Unknowns", selected: filter() == EventFilter::Unknowns, "Unknowns" }
                    option { value: "Blacklisted", selected: filter() == EventFilter::Blacklisted, "Blacklisted" }
                }
                button { onclick: export_csv, "Export CSV" }
                button { class: "secondary", onclick: export_json, "Export JSON" }
            }
            ul { class: "list",
                for event in filter_events(filter(), &events_data) {
                    li { "#{event.id} · {event.name} · conf {event.confidence:.2}" }
                }
                if filter_events(filter(), &events_data).is_empty() {
                    li { class: "muted", "No events for this filter" }
                }
            }
        }
    }
}

#[component]
fn Settings() -> Element {
    let mut detection_threshold = use_signal(|| 0.7);
    let mut recognition_threshold = use_signal(|| 0.85);
    let mut enable_tracking = use_signal(|| true);

    rsx! {
        header { class: "hero",
            div {
                h1 { "Settings" }
                p { "Configure detection, recognition, and system parameters" }
            }
        }

        section { class: "card",
            h3 { "Detection Settings" }
            div { style: "margin: 16px 0;",
                label { "Detection Confidence Threshold: {detection_threshold():.2}" }
                input {
                    r#type: "range",
                    min: "0",
                    max: "1",
                    step: "0.05",
                    value: "{detection_threshold()}",
                    oninput: move |e| {
                        if let Ok(val) = e.value().parse::<f64>() {
                            detection_threshold.set(val);
                        }
                    },
                    style: "width: 100%; margin-top: 8px;"
                }
            }
            div { style: "margin: 16px 0;",
                label { "Recognition Confidence Threshold: {recognition_threshold():.2}" }
                input {
                    r#type: "range",
                    min: "0",
                    max: "1",
                    step: "0.05",
                    value: "{recognition_threshold()}",
                    oninput: move |e| {
                        if let Ok(val) = e.value().parse::<f64>() {
                            recognition_threshold.set(val);
                        }
                    },
                    style: "width: 100%; margin-top: 8px;"
                }
            }
        }

        section { class: "card", style: "margin-top: 20px;",
            h3 { "Tracking Settings" }
            div { style: "margin: 16px 0;",
                label {
                    input {
                        r#type: "checkbox",
                        checked: enable_tracking(),
                        onchange: move |e| {
                            enable_tracking.set(e.checked());
                        },
                        style: "margin-right: 8px;"
                    }
                    "Enable face tracking"
                }
            }
        }

        section { class: "card", style: "margin-top: 20px;",
            h3 { "System Info" }
            ul { class: "list",
                li { "Platform: WASM" }
                li { "Version: 0.1.0" }
                li { "Core Engine: FaceGuard" }
            }
        }
    }
}

fn filter_events(filter: EventFilter, events: &[events::FaceEvent]) -> Vec<events::FaceEvent> {
    events
        .iter()
        .cloned()
        .filter(|event| match filter {
            EventFilter::All => true,
            EventFilter::Alerts => {
                matches!(event.event_type, events::EventType::Blacklisted | events::EventType::AfterHours)
                    || event.confidence < 0.4
            }
            EventFilter::Unknowns => event.event_type == events::EventType::UnknownFace,
            EventFilter::Blacklisted => event.event_type == events::EventType::Blacklisted,
        })
        .collect()
}

fn export_events_csv(filter: EventFilter, events: &[events::FaceEvent]) {
    let event_vec = filter_events(filter, events);
    let mut csv = String::from("id,name,confidence\n");
    for event in event_vec.iter() {
        csv.push_str(&format!("{},{},{}\n", event.id, event.name, event.confidence));
    }
    download_blob(&csv, "events.csv", "text/csv");
}

fn export_events_json(filter: EventFilter, events: &[events::FaceEvent]) {
    if let Ok(json) = serde_json::to_string(&filter_events(filter, events)) {
        download_blob(&json, "events.json", "application/json");
    }
}

fn download_blob(content: &str, filename: &str, _mime: &str) {
    if let Ok(blob) = Blob::new_with_str_sequence(&Array::of1(&JsValue::from_str(content))) {
        if let Ok(url) = Url::create_object_url_with_blob(&blob) {
            if let Some(window) = web_sys::window() {
                if let Some(document) = window.document() {
                    if let Ok(a) = document.create_element("a") {
                        let _ = a.set_attribute("href", &url);
                        let _ = a.set_attribute("download", filename);
                        if let Some(body) = document.body() {
                            let _ = body.append_child(&a);
                            if let Some(a_html) = a.dyn_ref::<HtmlElement>() {
                                a_html.click();
                            }
                            let _ = body.remove_child(&a);
                        }
                    }
                }
            }
            let _ = Url::revoke_object_url(&url);
        }
    }
}

async fn start_camera(video_id: &str) -> Result<(), JsValue> {
    let window = web_sys::window().ok_or_else(|| JsValue::from_str("No window"))?;
    let navigator = window.navigator();
    let media_devices = navigator
        .media_devices()
        .map_err(|err| JsValue::from(err))?;

    let constraints = MediaStreamConstraints::new();
    constraints.set_video(&JsValue::TRUE);

    let stream_js = JsFuture::from(
        media_devices
            .get_user_media_with_constraints(&constraints)
            .map_err(|err| JsValue::from(err))?,
    )
    .await?;

    let media_stream: MediaStream = stream_js.dyn_into()?;
    let document = window.document().ok_or_else(|| JsValue::from_str("No document"))?;
    let video: HtmlVideoElement = document
        .get_element_by_id(video_id)
        .ok_or_else(|| JsValue::from_str("No video element"))?
        .dyn_into()?;

    video.set_src_object(Some(&media_stream));
    video.set_muted(true);
    let _ = video.play();
    Ok(())
}

// Persistence helpers
fn load_identity_db() -> recognition::IdentityDatabase {
    LocalStorage::get(IDENTITY_DB_KEY).unwrap_or_else(|_| {
        let mut db = recognition::IdentityDatabase::new();
        // Add some demo identities
        db.add_identity("Demo User".to_string(), None);
        db
    })
}

fn save_identity_db(db: &recognition::IdentityDatabase) {
    let _ = LocalStorage::set(IDENTITY_DB_KEY, db);
}

fn load_event_log() -> events::EventLog {
    LocalStorage::get(EVENT_LOG_KEY).unwrap_or_else(|_| events::EventLog::new(1000))
}

fn save_event_log(log: &events::EventLog) {
    let _ = LocalStorage::set(EVENT_LOG_KEY, log);
}

fn load_settings() -> AppSettings {
    LocalStorage::get(SETTINGS_KEY).unwrap_or_default()
}

fn save_settings(settings: &AppSettings) {
    let _ = LocalStorage::set(SETTINGS_KEY, settings);
}

/// Detect faces from video element using simple edge detection
fn detect_faces_from_video(
    video_id: &str,
    canvas_id: &str,
) -> Option<Vec<detection::FaceDetection>> {
    let window = web_sys::window()?;
    let document = window.document()?;

    let video: HtmlVideoElement = document.get_element_by_id(video_id)?.dyn_into().ok()?;
    let canvas: HtmlCanvasElement = document.get_element_by_id(canvas_id)?.dyn_into().ok()?;

    let video_width = video.video_width();
    let video_height = video.video_height();

    if video_width == 0 || video_height == 0 {
        return None;
    }

    // Set canvas to match video
    canvas.set_width(video_width);
    canvas.set_height(video_height);

    let ctx: CanvasRenderingContext2d = canvas
        .get_context("2d")
        .ok()??
        .dyn_into()
        .ok()?;

    // Draw video frame to canvas
    ctx.draw_image_with_html_video_element(&video, 0.0, 0.0).ok()?;

    // Get image data
    let image_data = ctx
        .get_image_data(0.0, 0.0, video_width as f64, video_height as f64)
        .ok()?;

    // Simple face detection using brightness detection
    // In production, replace with actual ML model (MediaPipe, ONNX, etc.)
    let detections = simple_face_detection(&image_data, video_width, video_height);

    Some(detections)
}

/// Simple brightness-based face detection (placeholder for real ML model)
fn simple_face_detection(
    image_data: &ImageData,
    width: u32,
    height: u32,
) -> Vec<detection::FaceDetection> {
    let data = image_data.data();
    let mut detections = Vec::new();
    let mut detection_id = 1;

    // Divide frame into grid and look for face-like regions
    let grid_size = 64;
    let step_x = width / grid_size;
    let step_y = height / grid_size;

    for gy in 0..grid_size {
        for gx in 0..grid_size {
            let x = (gx * step_x) as f32;
            let y = (gy * step_y) as f32;
            let w = step_x as f32;
            let h = step_y as f32;

            // Sample brightness in this region
            let brightness = sample_brightness(&data, x as u32, y as u32, w as u32, h as u32, width);

            // Face-like regions typically have moderate brightness (skin tones)
            if brightness > 0.3 && brightness < 0.7 {
                // Check for face-like aspect ratio and variance
                if (w / h - 1.0).abs() < 0.5 {
                    // Calculate confidence based on how close to ideal skin tone
                    let confidence = 1.0 - ((brightness - 0.5).abs() * 2.0);
                    
                    if confidence > 0.6 {
                        detections.push(detection::FaceDetection::new(
                            detection_id,
                            x,
                            y,
                            w,
                            h,
                            confidence,
                        ));
                        detection_id += 1;
                    }
                }
            }
        }
    }

    detections
}

fn sample_brightness(data: &[u8], x: u32, y: u32, w: u32, h: u32, img_width: u32) -> f32 {
    let mut total = 0.0;
    let mut count = 0;

    for dy in 0..h.min(10) {
        for dx in 0..w.min(10) {
            let px = x + dx;
            let py = y + dy;
            
            if px < img_width && py < img_width {
                let idx = ((py * img_width + px) * 4) as usize;
                if idx + 2 < data.len() {
                    let r = data[idx] as f32;
                    let g = data[idx + 1] as f32;
                    let b = data[idx + 2] as f32;
                    total += (r + g + b) / 3.0 / 255.0;
                    count += 1;
                }
            }
        }
    }

    if count > 0 {
        total / count as f32
    } else {
        0.0
    }
}

fn draw_detections_and_tracks(
    video_id: &str,
    canvas_id: &str,
    detections: &[detection::FaceDetection],
    tracks: &[tracking::Track],
) {
    let window = web_sys::window();
    if window.is_none() {
        return;
    }
    let window = window.unwrap();
    
    let document = window.document();
    if document.is_none() {
        return;
    }
    let document = document.unwrap();

    let video_result: Result<HtmlVideoElement, JsValue> = document
        .get_element_by_id(video_id)
        .ok_or(JsValue::NULL)
        .and_then(|el| el.dyn_into());
    
    if video_result.is_err() {
        return;
    }
    let video = video_result.unwrap();

    let canvas_result: Result<HtmlCanvasElement, JsValue> = document
        .get_element_by_id(canvas_id)
        .ok_or(JsValue::NULL)
        .and_then(|el| el.dyn_into());
    
    if canvas_result.is_err() {
        return;
    }
    let canvas = canvas_result.unwrap();

    let video_width = video.video_width();
    let video_height = video.video_height();

    if video_width == 0 || video_height == 0 {
        return;
    }

    canvas.set_width(video_width);
    canvas.set_height(video_height);

    let ctx_result: Result<CanvasRenderingContext2d, JsValue> = canvas
        .get_context("2d")
        .map_err(|_| JsValue::NULL)?
        .ok_or(JsValue::NULL)
        .and_then(|v| v.dyn_into());
    
    if ctx_result.is_err() {
        return;
    }
    let ctx = ctx_result.unwrap();

    ctx.clear_rect(0.0, 0.0, video_width as f64, video_height as f64);

    // Draw tracks (tracked faces)
    ctx.set_stroke_style_str("#7b74ff");
    ctx.set_line_width(3.0);
    ctx.set_font("14px sans-serif");
    ctx.set_fill_style_str("#7b74ff");

    for track in tracks {
        let (x, y, w, h) = track.detection.bbox;
        
        ctx.stroke_rect(x as f64, y as f64, w as f64, h as f64);
        
        let label = format!("Track #{} ({:.0}%)", track.track_id, track.detection.confidence * 100.0);
        let text_y = if y > 25.0 { y - 8.0 } else { y + h + 20.0 };
        
        ctx.set_fill_style_str("rgba(123, 116, 255, 0.8)");
        ctx.fill_rect(x as f64, (text_y - 18.0) as f64, 150.0, 22.0);
        
        ctx.set_fill_style_str("#ffffff");
        let _ = ctx.fill_text(&label, x as f64 + 4.0, text_y as f64);
    }
}

